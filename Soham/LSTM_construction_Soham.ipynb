{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, K):\n",
    "        super().__init__()\n",
    "        \n",
    "        # K : input dimension\n",
    "        self.K = K\n",
    "        \n",
    "        # Let's make all hidden quantities K-dim for now\n",
    "        # i.e, i, f, o, z and c\n",
    "        # the decay rate is one-dimensional\n",
    "        \n",
    "        # Li = [W_i | U_i] in block form\n",
    "        # The input will vector will be [k | h]\n",
    "        \n",
    "        # For the lower limits of c\n",
    "        self.Li = nn.Linear(2*K, K)\n",
    "        self.Lf = nn.Linear(2*K, K)\n",
    "        \n",
    "        # For the upper limits of c\n",
    "        self.Libar = nn.Linear(2*K, K)\n",
    "        self.Lfbar = nn.Linear(2*K, K)\n",
    "        \n",
    "        self.Lz = nn.Linear(2*K, K)\n",
    "        self.Lo = nn.Linear(2*K, K)\n",
    "        self.Ld = nn.Linear(2*K, 1) # to predict the decay parameter delta\n",
    "        \n",
    "        # to predict lambda_tilde from h(t) - eqn.s (3a) ad (4a)\n",
    "        # This linear transformation has no bias\n",
    "        self.L_lamb_til = nn.Linear(K, K, bias=False)\n",
    "        \n",
    "        # Then, to predict lambda from lambda_tilde using softplus,\n",
    "        # we need scaling parameters\n",
    "        # we need to make these scales a part of the\n",
    "        # learnable parameter set\n",
    "        self.scale = nn.Parameter(pt.rand(K, requires_grad=True))\n",
    "        \n",
    "        # let's work with reLU for now\n",
    "        self.sigma = F.relu\n",
    "    \n",
    "    def forward(self, seq, times):\n",
    "        # seq : one hot encoded vectors of events (size N_events x K)\n",
    "        # times : times of occurences of the events (size N_events)\n",
    "        N_events = seq.shape[0]\n",
    "        \n",
    "        # Need to initialize the cell memories\n",
    "        # When nothing has occurred, the cell memories should be zero\n",
    "        ct = pt.zeros((self.K))\n",
    "        cbar = pt.zeros((self.K))\n",
    "        ht = pt.zeros((self.K))\n",
    "        \n",
    "        # Before event 0, there is no history\n",
    "        # So the output records predicted intensities \n",
    "        # of events 1 through N_events\n",
    "        out = pt.zeros(N_events - 1, K)\n",
    "        \n",
    "        # We also need the \"c\" values for the MC sampling\n",
    "        Clows = pt.zeros(N_events - 1, K)\n",
    "        Cbars = pt.zeros(N_events - 1, K)\n",
    "        \n",
    "        # Now let's propagate through the event sequence\n",
    "        # We'll go from event 0 to event N_events-1\n",
    "        # at each time index, the lambda values will be predicted\n",
    "        # for the next time index.\n",
    "        for evInd in range(N_events - 1):\n",
    "            \n",
    "            ev = pt.cat((seq[evInd], ht)).view(-1, K)\n",
    "            \n",
    "            # Now let's get the parameters\n",
    "            \n",
    "            # eqn (5a) and (5b) in the paper on page 5\n",
    "            i = self.sigma(self.Li(ev)).view(K)\n",
    "            f = self.sigma(self.Lf(ev)).view(K)\n",
    "            \n",
    "            # footnote (3) in the paper on page 5\n",
    "            ibar = self.sigma(self.Li(ev)).view(K)\n",
    "            fbar = self.sigma(self.Lf(ev)).view(K)\n",
    "            \n",
    "            # eqn (5c) and (5d) in the paper on page 5\n",
    "            z = self.sigma(self.Lz(ev)).view(K)\n",
    "            o = self.sigma(self.Lo(ev)).view(K)\n",
    "            \n",
    "            # eqn(6c) in the paper in the paper on page 5\n",
    "            delta = self.sigma(self.Ld(ev)).view(K)\n",
    "            \n",
    "            # Once these parameters are learned, we need\n",
    "            # to do the updates to c\n",
    "            \n",
    "            # eqn (6a) and (6b) in the paper on page 5\n",
    "            clow = f * ct + i * z\n",
    "            cbar = fbar * cbar + ibar * z\n",
    "            \n",
    "            # once clow, cbar and delta are found, we need to compute\n",
    "            # equation (7) in the paper\n",
    "            tnow = times[evInd] if evInd > 0 else 0\n",
    "            tnext = times[evInd + 1]\n",
    "            ct = cbar + (clow - cbar)*pt.exp((tnext - tnow)*delta)\n",
    "            \n",
    "            # with the c(t), we now have to determine h(t)\n",
    "            # eqn 4(b) on page 4 in the paper\n",
    "            ht = o * (2*self.sigma(2*ct) - 1)\n",
    "            \n",
    "            # Now, eqn. 4(a) linear part\n",
    "            lamb_til = self.L_lamb_til(ht.view(-1, K)).view(K)\n",
    "            \n",
    "            # Then the softplus part\n",
    "            # this will contain the event intensities for all the K events\n",
    "            # in the next step.\n",
    "            lamb = s * pt.log(1 + pt.exp(lamb_til / s))\n",
    "            \n",
    "            # Then record the event intensities for all the events\n",
    "            out[evInd, :] = lamb\n",
    "            \n",
    "            # Record the cell memories for the MC sampling\n",
    "            CLows[evInd, :] = clow\n",
    "            Cbars[evInd, :] = cbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
